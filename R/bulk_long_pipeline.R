#' Pipeline for Bulk Data
#'
#' @md
#' @description Semi-supervised isofrom detection and annotation for long read data.
#' This variant is meant for bulk samples. Specific parameters relating to
#' analysis can be changed either through function arguments, or through a
#' configuration JSON file.
#' @inherit sc_long_pipeline details description
#'
#' @return \code{bulk_long_pipeline} returns a SummarizedExperiment object, containing a count
#' matrix as an assay, gene annotations under metadata, as well as a list of the other
#' output files generated by the pipeline. The pipeline also outputs a number of output
#' files into the given \code{outdir} directory. These output files generated by the pipeline are:
#' \itemize{
#'  \item{transcript_count.csv.gz}{ - a transcript count matrix (also contained in the SummarizedExperiment)}
#'  \item{isoform_annotated.filtered.gff3}{ - isoforms in gff3 format (also contained in the SummarizedExperiment)}
#'  \item{transcript_assembly.fa}{ - transcript sequence from the isoforms}
#'  \item{align2genome.bam}{ - sorted BAM file with reads aligned to genome}
#'  \item{realign2transcript.bam}{ - sorted realigned BAM file using the transcript_assembly.fa as reference}
#'  \item{tss_tes.bedgraph}{ - TSS TES enrichment for all reads (for QC)}
#' }
#'
#' @param fastq the path to the directory containing the fastq input files to merge into one, `merged.fastq.gz`. If `merged.fastq.gz` already
#' exists, the fastq files are not merged and the existing merged file is used.
#' @param in_bam optional BAM file path which replaces fastq directory argument. This skips the genome alignment and
#' realignment steps
#' @inheritParams sc_long_pipeline
#'
#' @seealso
#' [sc_long_pipeline()] for single cell data,
#' [SummarizedExperiment()] for how data is outputted
#'
#' @example inst/examples/pipeline_example.R
#' @importFrom SummarizedExperiment SummarizedExperiment
#' @importFrom utils read.csv read.table
#' @export
bulk_long_pipeline <-
    function(annot,
             fastq,
             in_bam = NULL,
             outdir,
             genome_fa,
             minimap2_dir = "",
             downsample_ratio = 1,
             config_file = NULL,
             do_genome_align = TRUE,
             do_isoform_id = TRUE,
             do_read_realign = TRUE,
             do_transcript_quanti = TRUE,
             gen_raw_isoform = TRUE,
             has_UMI = FALSE,
             MAX_DIST = 10,
             MAX_TS_DIST = 100,
             MAX_SPLICE_MATCH_DIST = 10,
             min_fl_exon_len = 40,
             Max_site_per_splice = 3,
             Min_sup_cnt = 10,
             Min_cnt_pct = 0.01,
             Min_sup_pct = 0.2,
             strand_specific = 1,
             remove_incomp_reads = 5,
             use_junctions = TRUE,
             no_flank = TRUE,
             use_annotation = TRUE,
             min_tr_coverage = 0.75,
             min_read_coverage = 0.75) {
        # filenames for internal steps
        infq <- paste(outdir, "merged.fastq.gz", sep = "/")
        # bc_file <- paste(outdir, "pseudo_barcode_annotation.csv", sep="/")


        check_arguments(annot,
            fastq,
            in_bam,
            outdir,
            genome_fa,
            minimap2_dir,
            downsample_ratio,
            config_file,
            do_genome_align,
            do_isoform_id,
            do_read_realign,
            do_transcript_quanti,
            gen_raw_isoform,
            has_UMI,
            MAX_DIST,
            MAX_TS_DIST,
            MAX_SPLICE_MATCH_DIST,
            min_fl_exon_len,
            Max_site_per_splice,
            Min_sup_cnt,
            Min_cnt_pct,
            Min_sup_pct,
            strand_specific,
            remove_incomp_reads,
            use_junctions,
            no_flank,
            use_annotation,
            min_tr_coverage,
            min_read_coverage)

        # create output directory if one doesn't exist
        if (!dir.exists(outdir)) {
            cat("Output directory does not exists: one is being created\n")
            dir.create(outdir)
            print(outdir)
        }

        if (is.null(in_bam)) {
            # use existing merge fastq if already exists
            if (file.exists(infq)) {
                cat(infq, " already exists, no need to merge fastq files\n")
            } else {
                # this preprocessing needs only be done if we are using a fastq_dir, instead
                # of a bam file for reads,
                cat("Preprocessing bulk fastqs...\n")
                # run the merge_bulk_fastq function as preprocessing
                merge_bulk_fastq(fastq, infq)
            }
        } else {
            infq <- NULL
        }
        out_files <-
            generic_long_pipeline(
                annot,
                infq,
                in_bam,
                outdir,
                genome_fa,
                minimap2_dir,
                downsample_ratio,
                config_file,
                do_genome_align,
                do_isoform_id,
                do_read_realign,
                do_transcript_quanti,
                gen_raw_isoform,
                has_UMI,
                MAX_DIST,
                MAX_TS_DIST,
                MAX_SPLICE_MATCH_DIST,
                min_fl_exon_len,
                Max_site_per_splice,
                Min_sup_cnt,
                Min_cnt_pct,
                Min_sup_pct,
                strand_specific,
                remove_incomp_reads,
                use_junctions,
                no_flank,
                use_annotation,
                min_tr_coverage,
                min_read_coverage
            )

        se <- generate_bulk_summarized(out_files)

        # return the created summarizedexperiment
        se
    }

bulk_long_pipeline_cpp <-
    function(annot,
             fastq,
             in_bam = NULL,
             outdir,
             genome_fa,
             minimap2_dir = "",
             downsample_ratio = 1,
             config_file = NULL,
             do_genome_align = TRUE,
             do_isoform_id = TRUE,
             do_read_realign = TRUE,
             do_transcript_quanti = TRUE,
             gen_raw_isoform = TRUE,
             has_UMI = FALSE,
             MAX_DIST = 10,
             MAX_TS_DIST = 100,
             MAX_SPLICE_MATCH_DIST = 10,
             min_fl_exon_len = 40,
             Max_site_per_splice = 3,
             Min_sup_cnt = 10,
             Min_cnt_pct = 0.01,
             Min_sup_pct = 0.2,
             strand_specific = 1,
             remove_incomp_reads = 5,
             use_junctions = TRUE,
             no_flank = TRUE,
             use_annotation = TRUE,
             min_tr_coverage = 0.75,
             min_read_coverage = 0.75) {
        # filenames for internal steps
        infq <- paste(outdir, "merged.fastq.gz", sep = "/")
        # bc_file <- paste(outdir, "pseudo_barcode_annotation.csv", sep="/")


        check_arguments(annot,
            fastq,
            in_bam,
            outdir,
            genome_fa,
            minimap2_dir,
            downsample_ratio,
            config_file,
            do_genome_align,
            do_isoform_id,
            do_read_realign,
            do_transcript_quanti,
            gen_raw_isoform,
            has_UMI,
            MAX_DIST,
            MAX_TS_DIST,
            MAX_SPLICE_MATCH_DIST,
            min_fl_exon_len,
            Max_site_per_splice,
            Min_sup_cnt,
            Min_cnt_pct,
            Min_sup_pct,
            strand_specific,
            remove_incomp_reads,
            use_junctions,
            no_flank,
            use_annotation,
            min_tr_coverage,
            min_read_coverage)

        # create output directory if one doesn't exist
        if (!dir.exists(outdir)) {
            cat("Output directory does not exists: one is being created\n")
            dir.create(outdir)
            print(outdir)
        }

        if (is.null(in_bam)) {
            # use existing merge fastq if already exists
            if (file.exists(infq)) {
                cat(infq, " already exists, no need to merge fastq files\n")
            } else {
                # this preprocessing needs only be done if we are using a fastq_dir, instead
                # of a bam file for reads,
                cat("Preprocessing bulk fastqs...\n")
                # run the merge_bulk_fastq function as preprocessing
                merge_bulk_fastq(fastq, infq)
            }
        } else {
            infq <- NULL
        }
        out_files <-
            generic_long_pipeline_cpp(
                annot,
                infq,
                in_bam,
                outdir,
                genome_fa,
                minimap2_dir,
                downsample_ratio,
                config_file,
                do_genome_align,
                do_isoform_id,
                do_read_realign,
                do_transcript_quanti,
                gen_raw_isoform,
                has_UMI,
                MAX_DIST,
                MAX_TS_DIST,
                MAX_SPLICE_MATCH_DIST,
                min_fl_exon_len,
                Max_site_per_splice,
                Min_sup_cnt,
                Min_cnt_pct,
                Min_sup_pct,
                strand_specific,
                remove_incomp_reads,
                use_junctions,
                no_flank,
                use_annotation,
                min_tr_coverage,
                min_read_coverage
            )

        se <- generate_bulk_summarized(out_files)

        # return the created summarizedexperiment
        se
    }


generate_bulk_summarized <- function(out_files) {
    # change this to use out_files
    counts <- read.csv(out_files$counts)
    annot <- read.csv(out_files$annot, sep="\t", comment.char="#")
    colnames(annot) <-
        c(
            "SequenceID",
            "Source",
            "Feature",
            "Start",
            "End",
            "Score",
            "Strand",
            "Phase",
            "Attributes"
        )
    mdata <- list(
        "Annotations" = annot,
        "AnnotationFile" = out_files$annot,
        "OutputFiles" = out_files
    )
    se <-
        SummarizedExperiment::SummarizedExperiment(list("Flames Bulk" = counts),
            metadata = mdata
        )

    se
}
